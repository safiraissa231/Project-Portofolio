# -*- coding: utf-8 -*-
"""Assessment 1-Prompt Engineering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NmWZccseqM3yX_ulZRB7kxnmI2AjMYCI
"""

import json
import re
import requests
import streamlit as st

prompts = {
    "Mistral": "Summarize the key points discussed in the previous conversation in a brief paragraph in Bahasa Indonesia, not to exceed 300 words.",
    "Llama": "Provide a brief summary in Bahasa Indonesia of the preceding discussion, enclosed within triple backticks, with a maximum length of 300 words.",
    "Qwen": "Craft a succinct summary in Bahasa Indonesia of the conversation initiated by the user, limited to 300 words."
}

st.title("Open Source LLMs Model Demo")

def reset_state():
    for key in list(st.session_state.keys()):
        if key != 'api_key':
            del st.session_state[key]

with st.sidebar:
    if st.button('User Chat'):
        reset_state()
    selected_model = st.selectbox("Select Model", ['Mistral', 'Llama', 'Qwen'])

if 'api_key' not in st.session_state:
    api_key = st.sidebar.text_input('Enter API key:', type='password')
    if api_key:
        st.session_state['api_key'] = api_key
        st.sidebar.success('API key provided!', icon='âœ…')

def generate_completion(prompt, model, messages=None, temperature=0.7, max_tokens=2048, n=1, stop=None):
    endpoint = 'https://api.together.xyz/v1/completions'
    headers = {"Authorization": f"Bearer {st.session_state['api_key']}"}
    json_payload = {
        "model": model,
        "prompt": prompt,
        "messages": messages,
        'temperature': temperature,
        'max_tokens': max_tokens,
        'n': n,
        'stop': stop
    }
    response = requests.post(endpoint, json=json_payload, headers=headers)
    if response.status_code == 200:
        return response.json()
    else:
        st.error(f"Failed to generate summary. API responded with status code: {response.status_code}")
        return None

if 'api_key' in st.session_state:
    user_prompt = st.text_area("Enter your discussion:")
    if user_prompt:
        cleaned_prompt = re.sub(r"\s+", " ", user_prompt.strip())
        system_prompt = prompts.get(selected_model, "")
        complete_prompt = f"{system_prompt}\n\n{cleaned_prompt}" if system_prompt else cleaned_prompt

        model_identifiers = {
            "Mistral": "mistralai/Mistral-7B-Instruct-v0.2",
            "Llama": "togethercomputer/Llama-2-7B-32K-Instruct",
            "Qwen": "Qwen/Qwen1.5-1.8B-Chat"
        }

        model_name = model_identifiers.get(selected_model, "")
        response = generate_completion(complete_prompt, model_name, temperature=0.7, max_tokens=2048, n=1)

        if response and "choices" in response and response["choices"]:
            summary = response["choices"][0].get("text", "No summary generated.")
            st.write(summary)
        elif not response:
            st.write("No summary generated. Please check your API key and model selection.")